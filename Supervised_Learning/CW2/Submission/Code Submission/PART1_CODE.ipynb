{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If true, use the full dataset, otherwise use the toy dataset\n",
        "use_full_ds = True\n",
        "\n",
        "if use_full_ds:\n",
        "    raw_data = np.loadtxt('zipcombo.dat')\n",
        "else:\n",
        "    raw_data = np.vstack((np.loadtxt('dtrain123.dat'), np.loadtxt('dtest123.dat')))\n",
        "\n",
        "y_all = raw_data[:, 0]\n",
        "x_all = raw_data[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Main Implementation of Kernel Perceptron for Polynomial kernels used in Q1-Q6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KernelPerceptron:\n",
        "    \"\"\"\n",
        "    This is the implementation of the kernel perceptron for polynomial kernels. We intialize the paramteres and\n",
        "    then train the model by iterating through the data and updating the active indices and alpha. \n",
        "    \"\"\"\n",
        "    def __init__(self, d=1, epochs=5):\n",
        "        \"\"\"\n",
        "        Initialize the parameters where d is degree of the polynomial kernel, epochs is the number of, \n",
        "        active_indices contains the indices of the training samples which caused errors and alpha is the list containing the alpha values described in the report.\n",
        "        \"\"\"\n",
        "        self.d = d\n",
        "        self.epochs = epochs\n",
        "        self.active_indices = []\n",
        "        self.alpha = []\n",
        "    \n",
        "    def train(self, y_train, gram_mtx):  \n",
        "        # initialize the active indices and alpha\n",
        "        self.active_indices = []\n",
        "        self.alpha = []\n",
        "\n",
        "        # begin training loop\n",
        "        for _ in range(self.epochs):\n",
        "\n",
        "            # iterate over the labels\n",
        "            for i in range(len(y_train)):\n",
        "\n",
        "                # compute the score\n",
        "                score = 0.0\n",
        "                if len(self.active_indices) > 0:\n",
        "                    score = np.dot(self.alpha, gram_mtx[i, self.active_indices] ** self.d)\n",
        "                \n",
        "                # if the score is not the same as the label, add the sample to the active indices\n",
        "                if np.sign(score) != y_train[i]:\n",
        "                    self.active_indices.append(i)\n",
        "                    self.alpha.append(y_train[i])\n",
        "\n",
        "    def predict_scores(self, gram_test_train):\n",
        "        \"\"\"\n",
        "        This function computes the scores for the set.\n",
        "        \"\"\"\n",
        "        # if there are no active indices, return all zero to prevent any errors\n",
        "        if len(self.active_indices) == 0:\n",
        "            return np.zeros(gram_test_train.shape[0])\n",
        "\n",
        "        kvals = gram_test_train[:, self.active_indices] ** self.d\n",
        "        return np.dot(kvals, self.alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1 Solution is in the pdf portion of the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneVsRestClassifier:\n",
        "    \"\"\"\n",
        "    This is the one versus rest multi-class classifier using the kernel perceptron. The train function creates a binary classifier for each class.\n",
        "    The predict function then uses these binary classifiers to predict the class of a new instance.\n",
        "    \"\"\"\n",
        "    def __init__(self, d=1, epochs=5):\n",
        "        \"\"\"\n",
        "        We initialize the parameters where d is the degree of the polynomial kernel, epochs is the number of iterations, classifiers is the list of classifiers and \n",
        "        classes is the list of classes.\n",
        "        \"\"\"\n",
        "        self.d = d\n",
        "        self.epochs = epochs\n",
        "        self.classifiers = {}\n",
        "        self.classes = []\n",
        "        \n",
        "    def train(self, y_train, gram_full, train_indices):\n",
        "        # get unique classes and assign classifiers to an empty dictionary\n",
        "        self.classes = np.unique(y_train)\n",
        "        self.classifiers = {}\n",
        "\n",
        "        # construct the gram matrix for the training set\n",
        "        g_train = gram_full[np.ix_(train_indices, train_indices)]\n",
        "        \n",
        "        # iterate over the classes  \n",
        "        for i in self.classes:\n",
        "\n",
        "            # create the binary labels\n",
        "            y_labels = np.where(y_train == i, 1.0, -1.0)\n",
        "\n",
        "            # train the kernel perceptron\n",
        "            kp_class = KernelPerceptron(d=self.d, epochs=self.epochs)\n",
        "            kp_class.train(y_labels, g_train)\n",
        "            self.classifiers[i] = kp_class\n",
        "            \n",
        "    def predict(self, gram_full, test_indices, train_indices):\n",
        "        # get the gram matrix for the test and train set\n",
        "        g_test_train = gram_full[np.ix_(test_indices, train_indices)]\n",
        "\n",
        "        # init best scores and predictions\n",
        "        best_scores = np.full(len(test_indices), -np.inf)\n",
        "        predictions = np.zeros(len(test_indices))\n",
        "\n",
        "        # iterate over the classes\n",
        "        for c, kp in self.classifiers.items():\n",
        "            # compute the scores\n",
        "            scores = kp.predict_scores(g_test_train)\n",
        "\n",
        "            # select the best score\n",
        "            greater_than_best = scores > best_scores\n",
        "\n",
        "            # set the prediction\n",
        "            best_scores[greater_than_best] = scores[greater_than_best]\n",
        "            predictions[greater_than_best] = c\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting {run_count} runs...\n",
            "Q3 Results\n",
            "                 d=1            d=2            d=3            d=4            d=5            d=6            d=7\n",
            "Train  0.0778±0.0415  0.0115±0.0170  0.0032±0.0057  0.0017±0.0043  0.0004±0.0002  0.0003±0.0001  0.0003±0.0001\n",
            "Test   0.0967±0.0380  0.0406±0.0154  0.0312±0.0049  0.0289±0.0054  0.0286±0.0037  0.0273±0.0032  0.0281±0.0030\n"
          ]
        }
      ],
      "source": [
        "# precompute the gram matrix\n",
        "gram_full = np.dot(x_all, x_all.T)\n",
        "\n",
        "# initialize the parameters\n",
        "run_count = 20\n",
        "degrees = range(1, 8)\n",
        "n_samples = len(y_all)\n",
        "n_train = int(0.8 * n_samples)\n",
        "\n",
        "# initialize the training and test error arrays\n",
        "training_err = np.zeros((run_count, len(degrees)))\n",
        "test_err = np.zeros((run_count, len(degrees)))\n",
        "\n",
        "print(\"Starting {run_count} runs...\")\n",
        "\n",
        "# we perform 20 runs on the randomly split dataset.\n",
        "for i in range(run_count):\n",
        "    print(f\"Run {i+1}/{run_count}\", end=\"\\r\")\n",
        "\n",
        "    # randomize the sample indices\n",
        "    permutated = np.random.permutation(n_samples)\n",
        "\n",
        "    # split the data into train and test\n",
        "    train_idx = permutated[:n_train]\n",
        "    test_idx = permutated[n_train:]\n",
        "    \n",
        "    ytrain = y_all[train_idx]\n",
        "    ytest = y_all[test_idx]\n",
        "\n",
        "\n",
        "    # iterate over the polynomial degrees and train the model\n",
        "    for j, d in enumerate(degrees):\n",
        "\n",
        "        # instantiate and train the model\n",
        "        model = OneVsRestClassifier(d=d, epochs=5)\n",
        "        model.train(ytrain, gram_full, train_idx)\n",
        "        \n",
        "        # predict the training and test set\n",
        "        training_pred = model.predict(gram_full, train_idx, train_idx)\n",
        "        test_pred = model.predict(gram_full, test_idx, train_idx)\n",
        "        \n",
        "        # compute the training and test error\n",
        "        training_err[i, j] = np.mean(training_pred != ytrain)\n",
        "        test_err[i, j] = np.mean(test_pred != ytest)\n",
        "\n",
        "print(\"Q3 Results\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    f'd={d}': [f\"{np.mean(training_err[:, i]):.4f}±{np.std(training_err[:, i]):.4f}\",\n",
        "               f\"{np.mean(test_err[:, i]):.4f}±{np.std(test_err[:, i]):.4f}\"]\n",
        "    for i, d in enumerate(degrees)\n",
        "}, index=['Train', 'Test'])\n",
        "\n",
        "print(df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions 4, 5 and 6 : Cross-Validation, Confusion Matrix and Difficult to Classify Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running 20 runs with 5-fold CV...\n",
            "Q4 Results: Cross-Validation Hyperparameter Selection\n",
            "Mean d*: 5.15 ± 1.06\n",
            "Mean Train Error: 0.0006 ± 0.0004\n",
            "Mean Test Error:  0.0282 ± 0.0028\n",
            "Distribution of d*: {4: 7, 5: 6, 6: 4, 7: 3}\n",
            "Q5 Results: Confusion Matrix (Mean Error Rate ± Std)\n",
            "                       0            1            2            3            4            5            6            7            8            9\n",
            "True \\ Pred                                                                                                                                  \n",
            "0                      -  0.001±0.002  0.001±0.002  0.001±0.002  0.000±0.001  0.002±0.003  0.002±0.002  0.001±0.002  0.001±0.002  0.001±0.001\n",
            "1            0.000±0.001            -  0.001±0.001  0.000±0.001  0.002±0.003  0.000±0.000  0.002±0.003  0.002±0.003  0.001±0.002  0.000±0.001\n",
            "2            0.004±0.005  0.001±0.002            -  0.007±0.005  0.006±0.006  0.000±0.001  0.001±0.002  0.005±0.005  0.004±0.006  0.001±0.002\n",
            "3            0.004±0.005  0.001±0.002  0.006±0.006            -  0.001±0.002  0.017±0.014  0.000±0.000  0.003±0.004  0.012±0.009  0.003±0.004\n",
            "4            0.001±0.002  0.004±0.005  0.006±0.007  0.000±0.001            -  0.002±0.003  0.005±0.005  0.005±0.005  0.002±0.003  0.010±0.009\n",
            "5            0.007±0.006  0.001±0.002  0.006±0.005  0.013±0.014  0.005±0.005            -  0.005±0.007  0.001±0.003  0.005±0.006  0.005±0.006\n",
            "6            0.009±0.007  0.002±0.003  0.003±0.004  0.000±0.000  0.005±0.004  0.004±0.004            -  0.000±0.000  0.001±0.003  0.000±0.001\n",
            "7            0.001±0.002  0.002±0.003  0.003±0.004  0.001±0.002  0.006±0.009  0.000±0.000  0.000±0.000            -  0.003±0.003  0.008±0.005\n",
            "8            0.008±0.006  0.003±0.004  0.008±0.008  0.017±0.010  0.006±0.005  0.010±0.007  0.003±0.005  0.004±0.005            -  0.003±0.004\n",
            "9            0.001±0.002  0.001±0.006  0.002±0.003  0.001±0.002  0.013±0.011  0.001±0.003  0.000±0.001  0.011±0.007  0.001±0.003            -\n"
          ]
        }
      ],
      "source": [
        "def k_fold_cv(gram_full, train_indices, y_train, degrees, k=5):\n",
        "    \"\"\"\n",
        "    Performs k fold cross validation to select the best dimension. We iterate over the folds \n",
        "    where for each fold we train a model on the training set and evaluate it on the validation set. \n",
        "    We then return the best dimension. It takes in the gram matrix, the training indices, the training labels, \n",
        "    the polynomial degrees and the number of folds.\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples = len(train_indices)\n",
        "    fold_len = n_samples // k\n",
        "\n",
        "    # store indices for the subset of the data\n",
        "    sub_indices = np.arange(n_samples)\n",
        "\n",
        "    # shuffle the indices\n",
        "    np.random.shuffle(sub_indices)\n",
        "    \n",
        "    fold_errors = {d: [] for d in degrees}\n",
        "    \n",
        "    # iterate over the folds\n",
        "    for fold in range(k):\n",
        "\n",
        "        # get the start and end indices of the current fold\n",
        "        start = fold * fold_len\n",
        "        end = (fold + 1) * fold_len if fold < k - 1 else n_samples\n",
        "        \n",
        "        # create a mask for the validation set for current fold\n",
        "        val_mask = np.zeros(n_samples, dtype=bool)\n",
        "        val_mask[sub_indices[start:end]] = True\n",
        "        train_mask = ~val_mask\n",
        "        \n",
        "        # get the current train and validation indices and labels\n",
        "        curr_train_idx = train_indices[train_mask]\n",
        "        curr_val_idx = train_indices[val_mask]\n",
        "        curr_y_train = y_train[train_mask]\n",
        "        curr_y_val = y_train[val_mask]\n",
        "        \n",
        "        # iterate over the degrees and conclude the error rate for each degree\n",
        "        for d in degrees:\n",
        "            model = OneVsRestClassifier(d=d, epochs=5)\n",
        "            model.train(curr_y_train, gram_full, curr_train_idx)\n",
        "            preds = model.predict(gram_full, curr_val_idx, curr_train_idx)\n",
        "            fold_errors[d].append(np.mean(preds != curr_y_val))\n",
        "            \n",
        "    avg_errors = {d: np.mean(errs) for d, errs in fold_errors.items()}\n",
        "    return min(avg_errors, key=avg_errors.get)\n",
        "\n",
        "optimal_dim = []\n",
        "train_err_cv = []\n",
        "test_err_cv= []\n",
        "\n",
        "# setup confusion matrix for q5\n",
        "unique_classes = np.unique(y_all)\n",
        "class_map = {int(label): i for i, label in enumerate(unique_classes)}\n",
        "n_classes = len(unique_classes)\n",
        "conf_mtx = np.zeros((run_count, n_classes, n_classes))\n",
        "\n",
        "# initialize the variables needed for q6\n",
        "sample_test_counts = np.zeros(n_samples)\n",
        "sample_mistake_counts = np.zeros(n_samples)\n",
        "\n",
        "print(\"Running 20 runs with 5-fold CV...\")\n",
        "\n",
        "# perform 20 runs with 5 fold cross validation\n",
        "for run in range(run_count):\n",
        "    print(f\"Run {run+1}/{run_count}\", end=\"\\r\")\n",
        "    \n",
        "    # shuffle the indices and split the data\n",
        "    perm = np.random.permutation(n_samples)\n",
        "    train_idx = perm[:n_train]\n",
        "    test_idx = perm[n_train:]\n",
        "    y_train = y_all[train_idx]\n",
        "    y_test = y_all[test_idx]\n",
        "    \n",
        "    # run our k fold function to get the optimal dimension\n",
        "    best_dim = k_fold_cv(gram_full, train_idx, y_train, degrees, k=5)\n",
        "    optimal_dim.append(best_dim)\n",
        "    \n",
        "    # retrain the model with our optimal dimension\n",
        "    model = OneVsRestClassifier(d=best_dim, epochs=5)\n",
        "    model.train(y_train, gram_full, train_idx)\n",
        "    \n",
        "    # evaluate on training and test\n",
        "    pred_train = model.predict(gram_full, train_idx, train_idx)\n",
        "    pred_test = model.predict(gram_full, test_idx, train_idx)\n",
        "    \n",
        "    # track errors for q4\n",
        "    train_err_cv.append(np.mean(pred_train != y_train))\n",
        "    test_err_cv.append(np.mean(pred_test != y_test))\n",
        "    \n",
        "    # build confusion matrix for this run \n",
        "    for i in range(len(test_idx)):\n",
        "        true_label, pred_label = int(y_test[i]), int(pred_test[i])\n",
        "        if true_label != pred_label:\n",
        "            conf_mtx[run, class_map[true_label], class_map[pred_label]] += 1\n",
        "    \n",
        "    # normalize confusion matrix as described in the question\n",
        "    class_counts = np.zeros(n_classes)\n",
        "    for i in y_test:\n",
        "        class_counts[class_map[int(i)]] += 1\n",
        "    for i in range(n_classes):\n",
        "        if class_counts[i] > 0:\n",
        "            conf_mtx[run, i, :] /= class_counts[i]\n",
        "    \n",
        "    # track per-sample mistakes for q6\n",
        "    sample_test_counts[test_idx] += 1\n",
        "    sample_mistake_counts[test_idx[pred_test != y_test]] += 1\n",
        "\n",
        "\n",
        "## Q4 results\n",
        "print(\"Q4 Results: Cross-Validation Hyperparameter Selection\")\n",
        "print(f\"Mean d*: {np.mean(optimal_dim):.2f} ± {np.std(optimal_dim):.2f}\")\n",
        "print(f\"Mean Train Error: {np.mean(train_err_cv):.4f} ± {np.std(train_err_cv):.4f}\")\n",
        "print(f\"Mean Test Error:  {np.mean(test_err_cv):.4f} ± {np.std(test_err_cv):.4f}\")\n",
        "unique, counts = np.unique(optimal_dim, return_counts=True)\n",
        "print(f\"Distribution of d*: {dict(zip(unique, counts))}\")\n",
        "\n",
        "## Q5 results\n",
        "print(\"Q5 Results: Confusion Matrix (Mean Error Rate ± Std)\")\n",
        "mean_conf = np.mean(conf_mtx, axis=0)\n",
        "std_conf = np.std(conf_mtx, axis=0)\n",
        "\n",
        "confmtx_data = {}\n",
        "for j, pred in enumerate(unique_classes):\n",
        "    col = []\n",
        "    for i in range(n_classes):\n",
        "        if i == j:\n",
        "            col.append(\"-\")\n",
        "        else:\n",
        "            col.append(f\"{mean_conf[i,j]:.3f}±{std_conf[i,j]:.3f}\")\n",
        "    confmtx_data[int(pred)] = col\n",
        "\n",
        "confdf = pd.DataFrame(confmtx_data, index=[int(label) for label in unique_classes])\n",
        "confdf.index.name = \"True \\\\ Pred\"\n",
        "print(confdf.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q6 Results: Samples that were hardest to predict\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x300 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Q6 results\n",
        "print(\"Q6 Results: Samples that were hardest to predict\")\n",
        "\n",
        "# compute the mistake rates for the samples and get the top 5 hardest to predict samples\n",
        "valid_mask = sample_test_counts > 0\n",
        "mistake_rates = np.zeros(n_samples)\n",
        "mistake_rates[valid_mask] = sample_mistake_counts[valid_mask] / sample_test_counts[valid_mask]\n",
        "top_indices = np.argsort(mistake_rates)[-5:][::-1]\n",
        "\n",
        "\n",
        "q6_data = {\n",
        "    \"Rank\": [i+1 for i in range(5)],\n",
        "    \"Index\": top_indices,\n",
        "    \"True Label\": [int(y_all[idx]) for idx in top_indices],\n",
        "    \"Error Rate\": [f\"{mistake_rates[idx]:.0%}\" for idx in top_indices],\n",
        "    \"Mistakes/Tests\": [f\"{int(sample_mistake_counts[idx])}/{int(sample_test_counts[idx])}\" for idx in top_indices]\n",
        "}\n",
        "q6_df = pd.DataFrame(q6_data).set_index(\"Rank\")\n",
        "\n",
        "# visualize hardest samples as binary images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
        "\n",
        "for rank, idx in enumerate(top_indices[:5]):\n",
        "    ax = axes[rank]\n",
        "    matrix = x_all[idx].reshape(16, 16)\n",
        "    ax.imshow(matrix, cmap='gray_r')\n",
        "    ax.set_title(f\"Rank {rank+1}\\nIdx {idx}, Label: {int(y_all[idx])}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question 7a) is in the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7b): Running 20 runs for each c in [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]...\n",
            "7b) Results (Gaussian Kernel):\n",
            "         Train Error     Test Error\n",
            "c                                  \n",
            "0.001  0.0560±0.0143  0.0747±0.0155\n",
            "0.010  0.0005±0.0003  0.0280±0.0034\n",
            "0.050  0.0002±0.0002  0.0394±0.0052\n",
            "0.100  0.0001±0.0002  0.0553±0.0051\n",
            "0.500  0.0000±0.0000  0.0687±0.0071\n",
            "1.000  0.0000±0.0000  0.0699±0.0059\n",
            "5.000  0.0000±0.0000  0.0709±0.0058\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "7b)\n",
        "\"\"\"\n",
        "class GaussianKernelPerceptron:\n",
        "    \"\"\"\n",
        "    This is our implementation of the gaussian kernel perceptron. \n",
        "    This follows the same procedure as the polynomial kernel perceptron \n",
        "    but we redefine it here for clarity.\n",
        "    \"\"\"\n",
        "    def __init__(self, c=0.01, epochs=5):\n",
        "        self.c = c\n",
        "        self.epochs = epochs\n",
        "        self.active_indices = []\n",
        "        self.alpha = []\n",
        "    \n",
        "    def train(self, y_train, gram_mtx):\n",
        "        n_samples = len(y_train)\n",
        "        self.active_indices = []\n",
        "        self.alpha = []\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n_samples):\n",
        "                score = np.dot(self.alpha, gram_mtx[i, self.active_indices]) if self.active_indices else 0.0\n",
        "                if np.sign(score) != y_train[i]:\n",
        "                    self.active_indices.append(i)\n",
        "                    self.alpha.append(y_train[i])\n",
        "\n",
        "    def predict_scores(self, gram_test_train):\n",
        "        if not self.active_indices:\n",
        "            return np.zeros(gram_test_train.shape[0])\n",
        "        return np.dot(gram_test_train[:, self.active_indices], self.alpha)\n",
        "\n",
        "# find the squared distance matrix for the kernel so its precomputed\n",
        "diag_gram = np.diag(gram_full)\n",
        "sqdist = diag_gram[:, None] + diag_gram[None, :] - 2 * gram_full\n",
        "sqdist[sqdist < 0] = 0\n",
        "\n",
        "# initialize the parameters\n",
        "S = [0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]\n",
        "results_train_g = np.zeros((run_count, len(S)))\n",
        "results_test_g = np.zeros((run_count, len(S)))\n",
        "\n",
        "print(f\"7b): Running 20 runs for each c in {S}...\")\n",
        "\n",
        "# We redo the q3 procedure running OVR with the gaussian kernel\n",
        "for run in range(run_count):\n",
        "    print(f\"Run {run+1}/{run_count}\", end=\"\\r\")\n",
        "\n",
        "    # shuffle the indices\n",
        "    perm = np.random.permutation(n_samples)\n",
        "\n",
        "    # split the data into train and test\n",
        "    train_idx, test_idx = perm[:n_train], perm[n_train:]\n",
        "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
        "    \n",
        "    # iterate over all c values\n",
        "    for i, c in enumerate(S):\n",
        "\n",
        "        # compute the gaussian kernel\n",
        "        gram_train = np.exp(-c * sqdist[np.ix_(train_idx, train_idx)])\n",
        "        classifiers = {}\n",
        "\n",
        "        # train the model\n",
        "        for label in np.unique(y_train):\n",
        "            gk_class = GaussianKernelPerceptron(c=c, epochs=5)\n",
        "            gk_class.train(np.where(y_train == label, 1.0, -1.0), gram_train)\n",
        "            classifiers[label] = gk_class\n",
        "        \n",
        "        # initialize the best scores and predictions\n",
        "        best_scores = np.full(len(train_idx), -np.inf)\n",
        "        train_pred = np.zeros(len(train_idx))\n",
        "\n",
        "        # compute the scores for each class and select the best score\n",
        "        for label, gk_class in classifiers.items():\n",
        "            scores = gk_class.predict_scores(gram_train)\n",
        "            mask = scores > best_scores\n",
        "            best_scores[mask] = scores[mask]\n",
        "            train_pred[mask] = label\n",
        "\n",
        "        # compute the gaussian kernel for the test set\n",
        "        gram_test = np.exp(-c * sqdist[np.ix_(test_idx, train_idx)])\n",
        "\n",
        "        # initialize the best scores and predictions\n",
        "        best_scores = np.full(len(test_idx), -np.inf)\n",
        "        test_pred = np.zeros(len(test_idx))\n",
        "\n",
        "        # compute the scores for the test set and select the best score\n",
        "        for label, gk_class in classifiers.items():\n",
        "            scores = gk_class.predict_scores(gram_test)\n",
        "            mask = scores > best_scores\n",
        "            best_scores[mask] = scores[mask]\n",
        "            test_pred[mask] = label\n",
        "        \n",
        "        # compute the error rates for the train and test sets\n",
        "        results_train_g[run, i] = np.mean(train_pred != y_train)\n",
        "        results_test_g[run, i] = np.mean(test_pred != y_test)\n",
        "\n",
        "# output results\n",
        "q7b_df = pd.DataFrame({\n",
        "    'c': S,\n",
        "    'Train Error': [f\"{np.mean(results_train_g[:, i]):.4f}±{np.std(results_train_g[:, i]):.4f}\" for i in range(len(S))],\n",
        "    'Test Error': [f\"{np.mean(results_test_g[:, i]):.4f}±{np.std(results_test_g[:, i]):.4f}\" for i in range(len(S))]\n",
        "}).set_index('c')\n",
        "\n",
        "print(\"7b) Results (Gaussian Kernel):\")\n",
        "print(q7b_df.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7c): Running 20 runs with 5-fold CV for Gaussian kernel...\n",
            "7c) Results (Gaussian Kernel with CV):\n",
            "                     Value\n",
            "Metric                    \n",
            "Mean c*      0.0100±0.0000\n",
            "Train Error  0.0005±0.0003\n",
            "Test Error   0.0282±0.0040\n",
            "Distribution of c*: {0.01: 20}\n"
          ]
        }
      ],
      "source": [
        "# 7c) Guassian Kernel Cross Validation\n",
        "def k_fold_cv_gaussian(sqdist, train_indices, y_train, S, k=5):\n",
        "    \"\"\"\n",
        "    This is the implementation of cross validation for the gaussian kernel. The process \n",
        "    follows the same procedure that we did for the polynomial kernel.\n",
        "    \"\"\"\n",
        "\n",
        "    # prepare samples for cross validation\n",
        "    n_samples = len(train_indices)\n",
        "    fold_size = n_samples // k\n",
        "    sub_indices = np.arange(n_samples)\n",
        "    np.random.shuffle(sub_indices)\n",
        "    \n",
        "    fold_errors = {c: [] for c in S}\n",
        "    \n",
        "    # iterate over the folds\n",
        "    for fold in range(k):\n",
        "        start = fold * fold_size\n",
        "        end = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
        "\n",
        "        # create the train and validation masks\n",
        "        val_mask = np.zeros(n_samples, dtype=bool)\n",
        "        val_mask[sub_indices[start:end]] = True\n",
        "        train_mask = ~val_mask\n",
        "\n",
        "        # get the current train and validation indices as well as the labels for the iteration\n",
        "        curr_train_idx = train_indices[train_mask]\n",
        "        curr_val_idx = train_indices[val_mask]\n",
        "        curr_y_train = y_train[train_mask]\n",
        "        curr_y_val = y_train[val_mask]\n",
        "\n",
        "        # iterate over the c values that we chose\n",
        "        for c in S:\n",
        "            # compute the gaussian kernel\n",
        "            gram_train = np.exp(-c * sqdist[np.ix_(curr_train_idx, curr_train_idx)])\n",
        "            classifiers = {}\n",
        "\n",
        "            # train the model\n",
        "            for label in np.unique(curr_y_train):\n",
        "                gk_class = GaussianKernelPerceptron(c=c, epochs=5)\n",
        "                gk_class.train(np.where(curr_y_train == label, 1.0, -1.0), gram_train)\n",
        "                classifiers[label] = gk_class\n",
        "            \n",
        "            # compute the gaussian kernel for the validation set\n",
        "            gram_val = np.exp(-c * sqdist[np.ix_(curr_val_idx, curr_train_idx)])\n",
        "            best_scores = np.full(len(curr_val_idx), -np.inf)\n",
        "            test_pred = np.zeros(len(curr_val_idx))\n",
        "\n",
        "            # compute the scores for each class and select the best score\n",
        "            for label, gk_class in classifiers.items():\n",
        "                scores = gk_class.predict_scores(gram_val)\n",
        "                mask = scores > best_scores\n",
        "                best_scores[mask] = scores[mask]\n",
        "                test_pred[mask] = label\n",
        "\n",
        "            # compute the error rate for the validation set\n",
        "            fold_errors[c].append(np.mean(test_pred != curr_y_val))\n",
        "    \n",
        "    # compute the average error rate for each c\n",
        "    avg_err = {c: np.mean(errs) for c, errs in fold_errors.items()}\n",
        "    return min(avg_err, key=avg_err.get)\n",
        "\n",
        "# Cross validation over 20 runs with the gaussina kernel, we store the results in the below lists\n",
        "optimal_c_results = []\n",
        "cv_train_res = []\n",
        "cv_test_res = []\n",
        "\n",
        "print(\"7c): Running 20 runs with 5-fold CV for Gaussian kernel...\")\n",
        "\n",
        "# perform cross validation over 20 runs\n",
        "for run in range(run_count):\n",
        "    print(f\"Run {run+1}/{run_count}\", end=\"\\r\")\n",
        "\n",
        "    # shuffle the indices and split the data \n",
        "    perm = np.random.permutation(n_samples)\n",
        "    train_idx, test_idx = perm[:n_train], perm[n_train:]\n",
        "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
        "    \n",
        "    # perform CV to find the best c\n",
        "    optimal_c = k_fold_cv_gaussian(sqdist, train_idx, y_train, S, k=5)\n",
        "    optimal_c_results.append(optimal_c)\n",
        "    \n",
        "    # train the model with the best c\n",
        "    gram_train = np.exp(-optimal_c * sqdist[np.ix_(train_idx, train_idx)])\n",
        "    classifiers = {}\n",
        "    for label in np.unique(y_train):\n",
        "        gk_class = GaussianKernelPerceptron(c=optimal_c, epochs=5)\n",
        "        gk_class.train(np.where(y_train == label, 1.0, -1.0), gram_train)\n",
        "        classifiers[label] = gk_class\n",
        "    \n",
        "    # predict on the training set\n",
        "    best_scores = np.full(len(train_idx), -np.inf)\n",
        "    train_pred = np.zeros(len(train_idx))\n",
        "    for label, gk_class in classifiers.items():\n",
        "        scores = gk_class.predict_scores(gram_train)\n",
        "        mask = scores > best_scores\n",
        "        best_scores[mask] = scores[mask]\n",
        "        train_pred[mask] = label\n",
        "    \n",
        "    # predict the test set\n",
        "    gram_test = np.exp(-optimal_c * sqdist[np.ix_(test_idx, train_idx)])\n",
        "    best_scores = np.full(len(test_idx), -np.inf)\n",
        "    preds_te = np.zeros(len(test_idx))\n",
        "    for label, gk_class in classifiers.items():\n",
        "        scores = gk_class.predict_scores(gram_test)\n",
        "        mask = scores > best_scores\n",
        "        best_scores[mask] = scores[mask]\n",
        "        preds_te[mask] = label\n",
        "    \n",
        "    # compute the error rates for the training and test sets\n",
        "    cv_train_res.append(np.mean(train_pred != y_train))\n",
        "    cv_test_res.append(np.mean(preds_te != y_test))\n",
        "\n",
        "# Results table output\n",
        "unique, counts = np.unique(optimal_c_results, return_counts=True)\n",
        "\n",
        "q7c_df = pd.DataFrame({\n",
        "    'Metric': ['Mean c*', 'Train Error', 'Test Error'],\n",
        "    'Value': [\n",
        "        f\"{np.mean(optimal_c_results):.4f}±{np.std(optimal_c_results):.4f}\",\n",
        "        f\"{np.mean(cv_train_res):.4f}±{np.std(cv_train_res):.4f}\",\n",
        "        f\"{np.mean(cv_test_res):.4f}±{np.std(cv_test_res):.4f}\"\n",
        "    ]\n",
        "}).set_index('Metric')\n",
        "\n",
        "print(\"7c) Results (Gaussian Kernel with CV):\")\n",
        "print(q7c_df.to_string())\n",
        "print(f\"Distribution of c*: {dict(zip(unique, counts))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7d) Discussion is in the latex pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 8\n",
        "\n",
        "### 8a) Research is in the latex pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8b): Running 20 runs with OvO...\n",
            "8b) Results (OvO):\n",
            "Degree   | Train Error          | Test Error          \n",
            "1        | 0.0424 ± 0.0091       | 0.0703 ± 0.0104\n",
            "2        | 0.0077 ± 0.0028       | 0.0413 ± 0.0040\n",
            "3        | 0.0037 ± 0.0034       | 0.0362 ± 0.0048\n",
            "4        | 0.0016 ± 0.0026       | 0.0340 ± 0.0063\n",
            "5        | 0.0011 ± 0.0012       | 0.0338 ± 0.0037\n",
            "6        | 0.0007 ± 0.0006       | 0.0332 ± 0.0046\n",
            "7        | 0.0011 ± 0.0023       | 0.0364 ± 0.0052\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "8b)\n",
        "\"\"\"\n",
        "class OneVsOneClassifier:\n",
        "    \"\"\"\n",
        "    This is the implementation of the one versus one classifier as described in the latex pdf.\n",
        "    \"\"\"\n",
        "    def __init__(self, d=1, epochs=5):\n",
        "        \"\"\"\n",
        "        Initialize the parameters where d is the degree of the polynomial kernel, epochs is the number of epochs, \n",
        "        classifiers is a dictionary of the classifiers for each pair of classes and classes is the list of classes.\n",
        "        \"\"\"\n",
        "        self.d = d\n",
        "        self.epochs = epochs\n",
        "        self.classifiers = {}\n",
        "        self.classes = []\n",
        "        \n",
        "    def train(self, y_train, gram_full, train_indices):\n",
        "        self.classes = np.unique(y_train)\n",
        "        self.classifiers = {}\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # double for loop to iterate over all pairs of classes\n",
        "        for i in range(n_classes):\n",
        "            for j in range(i + 1, n_classes):\n",
        "\n",
        "                # get relevant pair of classes and the mask for that pair\n",
        "                c1, c2 = self.classes[i], self.classes[j]\n",
        "                mask_pair = (y_train == c1) | (y_train == c2)\n",
        "                local_idx = np.where(mask_pair)[0]\n",
        "\n",
        "                # get the global indices of the pair of classes\n",
        "                subset_global_idx = train_indices[local_idx]\n",
        "                \n",
        "                # get the binary labels for the pair of classes\n",
        "                y_binary = np.where(y_train[local_idx] == c1, 1.0, -1.0)\n",
        "\n",
        "                # get the gram matrix for the pair of classes\n",
        "                gram_pair = gram_full[np.ix_(subset_global_idx, subset_global_idx)]\n",
        "                \n",
        "                # train the kernel perceptron for the pair of classes\n",
        "                kp_class = KernelPerceptron(d=self.d, epochs=self.epochs)\n",
        "                kp_class.train(y_binary, gram_pair)\n",
        "                \n",
        "                self.classifiers[(c1, c2)] = (kp_class, subset_global_idx)\n",
        "\n",
        "    def predict(self, gram_full, test_indices):\n",
        "        \"\"\"\n",
        "        The predict function implements the voting mechanism for the OVO\n",
        "        classifier. It takes in the gram matrix and the test indices and returns\n",
        "        the predicted class.\n",
        "        \"\"\"\n",
        "        n_test = len(test_indices)\n",
        "        votes = np.zeros((n_test, len(self.classes)))\n",
        "\n",
        "        # this is the mapping of classes to their index which we will use for voting \n",
        "        class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "        \n",
        "        # iterate over all the classifiers\n",
        "        for (c1, c2), (kp, subset_global_idx) in self.classifiers.items():\n",
        "            # get the gram matrix for the test pair\n",
        "            gram_pair = gram_full[np.ix_(test_indices, subset_global_idx)]\n",
        "            # get the predictions and deterministically select 1 in the case of a tie\n",
        "            predictions = np.where(kp.predict_scores(gram_pair)>= 0, 1.0, -1.0)\n",
        "            # vote for the class\n",
        "            votes[predictions > 0, class_to_idx[c1]] += 1\n",
        "            votes[predictions < 0, class_to_idx[c2]] += 1\n",
        "        # return the class with the most votes\n",
        "        return self.classes[np.argmax(votes, axis=1)]\n",
        "\n",
        "ovo_train_results = np.zeros((run_count, len(degrees)))\n",
        "ovo_test_results = np.zeros((run_count, len(degrees)))\n",
        "\n",
        "print(\"8b): Running 20 runs with OvO...\")\n",
        "\n",
        "# iterate over 20 runs and train each of the classifiers\n",
        "for run in range(run_count):\n",
        "    print(f\"Run {run+1}/{run_count}\", end=\"\\r\")\n",
        "    \n",
        "    # as we do in the other questions, suffle the data and split it into train and test\n",
        "    perm = np.random.permutation(n_samples)\n",
        "    train_idx, test_idx = perm[:n_train], perm[n_train:]\n",
        "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
        "    \n",
        "    # iterate of all the possible degrees and train the models\n",
        "    for i, d in enumerate(degrees):\n",
        "        model = OneVsOneClassifier(d=d, epochs=5)\n",
        "        model.train(y_train, gram_full, train_idx)\n",
        "        ovo_train_results[run, i] = np.mean(model.predict(gram_full, train_idx) != y_train)\n",
        "        ovo_test_results[run, i] = np.mean(model.predict(gram_full, test_idx) != y_test)\n",
        "\n",
        "print(\"8b) Results (OvO):\")\n",
        "print(f\"{'Degree':<8} | {'Train Error':<20} | {'Test Error':<20}\")\n",
        "for i, d in enumerate(degrees):\n",
        "    mt, st = np.mean(ovo_train_results[:, i]), np.std(ovo_train_results[:, i])\n",
        "    me, se = np.mean(ovo_test_results[:, i]), np.std(ovo_test_results[:, i])\n",
        "    print(f\"{d:<8} | {mt:.4f} ± {st:.4f}       | {me:.4f} ± {se:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8c): Running 20 runs with 5-fold CV for OvO...\n",
            "8c) Results (OvO with CV):\n",
            "Mean d*: 4.85 ± 1.01\n",
            "Mean Train Error: 0.0009 ± 0.0008\n",
            "Mean Test Error:  0.0315 ± 0.0034\n",
            "Distribution of d*: {3: 3, 4: 3, 5: 8, 6: 6}\n"
          ]
        }
      ],
      "source": [
        "## 8c)\n",
        "def k_fold_cv_ovo(gram_full, train_indices, y_train, degrees, k=5):\n",
        "    \"\"\"\n",
        "    This is the implementation of cross validation for the one versus one classifier. This is a similar approach to the \n",
        "    approach we used in previous questions.\n",
        "    \"\"\"\n",
        "    n_samples = len(train_indices)\n",
        "    fold_size = n_samples // k\n",
        "    sub_indices = np.arange(n_samples)\n",
        "    np.random.shuffle(sub_indices)\n",
        "    \n",
        "    fold_errors = {d: [] for d in degrees}\n",
        "    \n",
        "    for fold in range(k):\n",
        "        start = fold * fold_size\n",
        "        end = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
        "        \n",
        "        # create the validation and train masks\n",
        "        val_mask = np.zeros(n_samples, dtype=bool)\n",
        "        val_mask[sub_indices[start:end]] = True\n",
        "        train_mask = ~val_mask\n",
        "        \n",
        "        # get the current train and validation indices and labels\n",
        "        curr_train_idx = train_indices[train_mask]\n",
        "        curr_val_idx = train_indices[val_mask]\n",
        "        curr_y_train = y_train[train_mask]\n",
        "        curr_y_val = y_train[val_mask]\n",
        "        \n",
        "        # iterate over all the degrees and train each model\n",
        "        for d in degrees:\n",
        "            model = OneVsOneClassifier(d=d, epochs=5)\n",
        "            model.train(curr_y_train, gram_full, curr_train_idx)\n",
        "            preds = model.predict(gram_full, curr_val_idx)\n",
        "            fold_errors[d].append(np.mean(preds != curr_y_val))\n",
        "            \n",
        "    avg_errors = {d: np.mean(errs) for d, errs in fold_errors.items()}\n",
        "    return min(avg_errors, key=avg_errors.get)\n",
        "\n",
        "\n",
        "# initialize the lists which will store the optimal dimensions and the error rates for training and testing.\n",
        "optimal_dim_ovo = []\n",
        "train_ovo_results = []\n",
        "test_ovo_results = []\n",
        "\n",
        "print(\"8c): Running 20 runs with 5-fold CV for OvO...\")\n",
        "\n",
        "# iterate over 20 runs rerunning the Q4 procedure using the OvO classifier\n",
        "for run in range(run_count):\n",
        "    print(f\"Run {run+1}/{run_count}\", end=\"\\r\")\n",
        "\n",
        "    # prepare the data\n",
        "    perm = np.random.permutation(n_samples)\n",
        "    train_idx, test_idx = perm[:n_train], perm[n_train:]\n",
        "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
        "    \n",
        "    # run the cross validation to find the best d\n",
        "    optimal_dim = k_fold_cv_ovo(gram_full, train_idx, y_train, degrees, k=5)\n",
        "    optimal_dim_ovo.append(optimal_dim)\n",
        "    \n",
        "    # retrain with the best dimension\n",
        "    model = OneVsOneClassifier(d=optimal_dim, epochs=5)\n",
        "    model.train(y_train, gram_full, train_idx)\n",
        "    \n",
        "    # predict the train and test set\n",
        "    pred_train = model.predict(gram_full, train_idx)\n",
        "    pred_test = model.predict(gram_full, test_idx)\n",
        "    \n",
        "    # compute the error rates for the train and test sets\n",
        "    train_ovo_results.append(np.mean(pred_train != y_train))\n",
        "    test_ovo_results.append(np.mean(pred_test != y_test))\n",
        "\n",
        "print(\"8c) Results (OvO with CV):\")\n",
        "print(f\"Mean d*: {np.mean(optimal_dim_ovo):.2f} ± {np.std(optimal_dim_ovo):.2f}\")\n",
        "print(f\"Mean Train Error: {np.mean(train_ovo_results):.4f} ± {np.std(train_ovo_results):.4f}\")\n",
        "print(f\"Mean Test Error:  {np.mean(test_ovo_results):.4f} ± {np.std(test_ovo_results):.4f}\")\n",
        "unique, counts = np.unique(optimal_dim_ovo, return_counts=True)\n",
        "print(f\"Distribution of d*: {dict(zip(unique, counts))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8d) Discussion is in the latex pdf"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
