{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d137e5c5",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7fccb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2aa1f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6ff8333decd45fafeeecca8f8df44a9",
     "grade": false,
     "grade_id": "cell-51e5da9ddd4cc99c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.distributions as dist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94413c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a85be7c9bda094c47c477f61da31e5a3",
     "grade": false,
     "grade_id": "cell-a4d703cf4d20ee02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Bayesian linear classifiers\n",
    "\n",
    "In this example you are going to fit a Bayesian logistic regression model, using different sets of features on data $\\mathbf{x} \\in \\mathbb{R}^2$.\n",
    "\n",
    "The first one is a \"simple\" set of features, with just the two input features plus an intercept:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{\\text{simple}}(\\mathbf{x}) &= [ 1, x_1, x_2 ]\n",
    "\\end{align*}$$\n",
    "\n",
    "The second is a set of quadratic polynomial features,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{\\text{quadratic}}(\\mathbf{x}) &= [ 1, x_1, x_2, x_1x_2, x_1^2, x_2^2 ].\n",
    "\\end{align*}$$\n",
    "\n",
    "Beyond that, it is up to you! You will have to choose features that you think are appropriate for the data.\n",
    "This example just includes quadratic terms, but you could also include higher order polynomial terms (e.g. $x_1^3, x_2^3$) or completely different features alltogether. \n",
    "It's up to you — but you should be prepared to justify your choice!\n",
    "\n",
    "Then, in the last section, you will try using deep learning to learn features (instead of hand-crafting them) — you can then compare these features and predictions and consider what is really best.\n",
    "\n",
    "You will also fit the model in two different ways:\n",
    "\n",
    "1. MAP estimation (penalized maximum likelihood)\n",
    "2. Laplace approximation (a Gaussian approximate posterior, centered at the mode)\n",
    "\n",
    "Here is a synthetic dataset that we'll be working with (plotting the training set only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecb2d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f166f72bf580d2a83ea40961e1210040",
     "grade": false,
     "grade_id": "cell-07ecbcc2cafce227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset, validation_set = torch.load(\"two_moons.pt\", weights_only=False)\n",
    "X_train, y_train = dataset.tensors\n",
    "X_test, y_test = validation_set.tensors\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ca843",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbe785c96ee869ea03a5b51eacf4b3d9",
     "grade": false,
     "grade_id": "cell-791ab24aadbc08a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here are definitions of two different feature maps, the \"simple\" one and the \"quadratic\" one.\n",
    "\n",
    "They define feature spaces in $\\mathbb{R}^3$ and $\\mathbb{R}^6$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f254ff7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a166ea18b83bb7485d4fc6453b80b721",
     "grade": false,
     "grade_id": "cell-318dc4fe1089d515",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features_simple(X):\n",
    "    return torch.concat((torch.ones_like(X[:,:1]), X), -1)\n",
    "\n",
    "def features_quadratic(X):\n",
    "    interactions = X.prod(-1, keepdim=True)\n",
    "    return torch.concat((torch.ones_like(X[:,:1]), \n",
    "                         X, X.pow(2), interactions), -1)\n",
    "\n",
    "print(\"Dimension of Phi, `features_simple`:\", features_simple(X_train).shape)\n",
    "print(\"Dimension of Phi, `features_quadratic`:\", features_quadratic(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2e543",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "268279fdb7e070742cb5496dee1f8d7b",
     "grade": false,
     "grade_id": "cell-390950f2d4a30066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #1 (3 points): Define the model\n",
    "\n",
    "The Bayesian logistic regression model we are working with has the form\n",
    "$$\\begin{align*}\n",
    "\\mathbf{w} &\\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}) \\\\\n",
    "\\hat y_i &= \\mathrm{Logistic}(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)) \\\\\n",
    "y_i &\\sim \\mathrm{Bernoulli}(\\hat y_i)\n",
    "\\end{align*}$$\n",
    "where $i = 1,\\dots, N$ and the Logistic function is defined\n",
    "$$\\begin{align*}\n",
    "\\mathrm{Logistic}(z) &= \\frac{1}{1 + \\exp\\{-z\\}}.\n",
    "\\end{align*}$$\n",
    "It's implemented in pytorch as `torch.sigmoid`.\n",
    "\n",
    "The first step is to define two functions, one to make predictions given a weight vector $\\mathbf{w}$ and inputs $\\Phi$, and one which computes the log joint probability\n",
    "\n",
    "$$\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}, \\sigma^2).$$\n",
    "\n",
    "I've done the first one for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477dc07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e5e5517d0441c8d5c12c738d656ab4a",
     "grade": false,
     "grade_id": "cell-2bfc837bc01b0b20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_probs_MAP(Phi, w):\n",
    "    \"\"\"\n",
    "    Given a \"design matrix\" Phi, and a point estimate w, compute p(y = 1 | Phi, w)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    w     : (D,) vector of weights\n",
    "\n",
    "    OUTPUT:\n",
    "    y_hat : (N,) vector of probabilities p(y=1 | Phi, w)\n",
    "    \"\"\"\n",
    "    return torch.sigmoid(Phi @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fc311",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8254dfaca75f72cdb1b6644868222c7b",
     "grade": false,
     "grade_id": "B-log-joint",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_joint(Phi, y, w, sigma=10):\n",
    "    \"\"\"\n",
    "    Compute the joint probability of the data and the latent variables.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w     : (D,) vector of weights\n",
    "    sigma : scalar, standard deviation of Gaussian prior distribution p(w).\n",
    "            Leave this set to sigma=10 for purposes of this exercise\n",
    "\n",
    "    OUTPUT:\n",
    "    log_joint : the log probability log p(y, w | Phi, sigma), a torch scalar\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a307f74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7414775d4662069e3ed76e8c157c3663",
     "grade": true,
     "grade_id": "B-joint-test-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478df225",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56476349ca26272aa489be1492bc6d3b",
     "grade": true,
     "grade_id": "B-joint-test-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc5535",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3462977d3068940b7acfa46272e66ea",
     "grade": false,
     "grade_id": "cell-91a0bc31c0fe7b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK 2 (5 points): Implement MAP estimation\n",
    "\n",
    "Now you need to write a function which performs MAP estimation, i.e. penalized maximum likelihood estimation.\n",
    "\n",
    "This function should find the value $\\mathbf{w}_{MAP}$ that maximizes the log joint, i.e.\n",
    "\n",
    "$$\\mathbf{w}_{MAP} = \\mathrm{argmax}_{\\mathbf{w}}\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}, \\sigma^2).$$\n",
    "\n",
    "To do this, you should **use pytorch autograd tools**. This will involve defining an initial value of the weights, computing a scalar loss function, and calling `.backward()`, and then performing gradient-based optimization. Take a look at the demo notebooks from previous lectures for examples…!\n",
    "\n",
    "* You **may feel free to use classes from `torch.optim`**. I would suggest the use of `torch.optim.SGD` or `torch.optim.Adagrad`.\n",
    "* Regardless of how you do this, you will need to decide on a stopping criteria for your optimization routine.\n",
    "* You will also need to decide on how to set the parameters (learning rate, momentum, anything else!) for your selected optimizer.\n",
    "\n",
    "Also, **your code should work for ANY features!**. We will test this out on not just the \"simple\" and \"quadratic\" features above, but also on your own custom choice of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed4742",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0136517993392d94f124c530ccf7f33",
     "grade": false,
     "grade_id": "B-find-MAP",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_MAP(Phi, y):\n",
    "    \"\"\"\n",
    "    Find the MAP estimate of the log_joint method.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    w      : (D,) vector of optimized weights\n",
    "    losses : list of losses at each iteration of the optimization algorithm.\n",
    "             Should be a list of scalars, which can be plotted afterward to\n",
    "             diagnose convergence.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = torch.zeros(Phi.shape[1]).requires_grad_(True)\n",
    "    losses = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return weights.detach(), losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c9cf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9db727380135210315c5bf49f0c0e9",
     "grade": false,
     "grade_id": "cell-0bb96b3160b8580d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following two cells call `find_MAP` to compute $\\mathbf{w}$ for both choices of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_simple, losses = find_MAP(features_simple(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_quad, losses = find_MAP(features_quadratic(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363aeaa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d4352ba194de7f7d86b9ccc051fc9e4",
     "grade": true,
     "grade_id": "B-test-MAP-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927c5f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5505930918b72a9db67ba7e1f3f843",
     "grade": true,
     "grade_id": "B-test-MAP-2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9a9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c088909096a75d2f55ad28f00f4b7db",
     "grade": false,
     "grade_id": "cell-f4589a3dbb0d0545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualization: The following code visualizes the classifier result\n",
    "\n",
    "It plots the probability of being one class or the other using a color contour plot.\n",
    "\n",
    "The decision boundary is a dashed black line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(X, y, pred):\n",
    "    h = 0.2\n",
    "    x_min, x_max = -5, 5\n",
    "    y_min, y_max = -3, 3\n",
    "    xx, yy = np.meshgrid(np.arange(float(x_min), float(x_max), h),\n",
    "                         np.arange(float(y_min), float(y_max), h))\n",
    "\n",
    "    Z = pred(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=.8, levels=np.linspace(0, 1, 8), cmap='autumn')\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');\n",
    "    plt.contour(xx, yy, Z, levels=(0.5,), linestyles='dashed');\n",
    "    \n",
    "    # Plot the testing points\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_simple(X), w_MAP_simple))\n",
    "plt.title(\"MAP estimate, simple features\")\n",
    "train_accuracy = (predict_probs_MAP(features_simple(X_train), w_MAP_simple).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_simple(X_test), w_MAP_simple).round() == y_test).float().mean()\n",
    "print(\"Simple features: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(X_train, y_train,\n",
    "              lambda X: predict_probs_MAP(features_quadratic(X), w_MAP_quad))\n",
    "plt.title(\"MAP estimate, quadratic features\")\n",
    "train_accuracy = (predict_probs_MAP(features_quadratic(X_train), w_MAP_quad).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_quadratic(X_test), w_MAP_quad).round() == y_test).float().mean()\n",
    "print(\"Quadratic features: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51394eb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cf08545253362f4241e4e2387a83f88",
     "grade": false,
     "grade_id": "cell-3166631d224980bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #3 (4 points): Laplace approximation\n",
    "\n",
    "In the next section, you will fit an approximate posterior over the weights by using the Laplace approximation around the mode $\\mathbf{w}_{MAP}$ of the distribution you found above.\n",
    "\n",
    "This requires completing two functions:\n",
    "\n",
    "1. `compute_laplace_Cov` takes the data and the MAP estimate, and outputs a covariance matrix defined as the negative inverse Hessian of the log target density. (See the week 4 lecture slides for details on how to compute this!)\n",
    "2. `predict_bayes` makes predictions on new data points, by approximating $\\int p(y | x, w)p(w | \\mathcal{D})dw$ when using a Gaussian approximation to $p(w | \\mathcal{D})$. In the week 4 lecture slides we discussed three different ways of computing this — it is up to you to decide what method you would prefer, and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b04a17",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3cedcf986e87e777c1ba12c5c6c2443",
     "grade": false,
     "grade_id": "B-laplace-cov",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_laplace_Cov(Phi, y, w_MAP, sigma=10):\n",
    "    \"\"\"\n",
    "    Compute the Laplace approximation of the posterior covariance \n",
    "    in a logistic regression setting.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    sigma : scalar, standard deviation of Gaussian prior distribution p(w).\n",
    "            Leave this set to sigma=10 for purposes of this exercise\n",
    "\n",
    "    OUTPUT:\n",
    "    Cov : (D, D) posterior covariance matrix estimate defined by the Laplace \n",
    "          approximation\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da31180",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "532ca1f2a118acd58c737ec7545cd821",
     "grade": false,
     "grade_id": "B-laplace-predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_bayes(Phi, w_MAP, Cov):\n",
    "    \"\"\"\n",
    "    Make predictions on new data points using an approximate posterior \n",
    "    w ~ MultivariateNormal(w_MAP, Cov)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    Cov   : (D, D) approximate posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    y_hat : (N,) vector of probabilities p(y=1 | Phi)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2c239",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b20317869f5427299c73a7aed504e984",
     "grade": false,
     "grade_id": "cell-e563aca374ddf46f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following cells call your functions above to compute the Laplace approximation and visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e440e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13417e655e2239c0b67ca963c6d3d70a",
     "grade": false,
     "grade_id": "cell-9d607a490ff71c7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Cov_simple = compute_laplace_Cov(features_simple(X_train), y_train, w_MAP_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Simple features: MAP\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_simple(X), w_MAP_simple))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Simple features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_simple(X), w_MAP_simple, Cov_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adf170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6b7e143516c6b3c557c44f5aeb06d8",
     "grade": false,
     "grade_id": "cell-e6516c087b43b9a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Cov_quad = compute_laplace_Cov(features_quadratic(X_train), y_train, w_MAP_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Quadratic features: MAP\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_quadratic(X), w_MAP_quad))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Quadratic features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_quadratic(X), w_MAP_quad, Cov_quad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e528",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0414b2f798b743821e7a3cfade2dbc44",
     "grade": true,
     "grade_id": "B-test-Laplace-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f87376",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94674dc5dd062cf200627a4c8cac432e",
     "grade": true,
     "grade_id": "B-test-Laplace-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5380",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "264d3021a237270917668394c5505e58",
     "grade": true,
     "grade_id": "B-test-Laplace-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567389d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e965d127f726030eaa6252f83ee6a35d",
     "grade": false,
     "grade_id": "cell-303212e96a305a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #4 (2 points): Model comparison\n",
    "\n",
    "You can compute the marginal likelihood approximation defined by the Laplace approximation.\n",
    "\n",
    "This estimate of the evidence can be used, even just looking at the training data, to help decide which of the two feature maps is more appropriate and better fits the data.\n",
    "\n",
    "This can help guard against potential overfitting if using features that are \"too complex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dd545",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34fc8b4b795c39c977b6c55e09e2af59",
     "grade": false,
     "grade_id": "B-laplace-evidence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_laplace_log_evidence(Phi, y, w_MAP, Cov):\n",
    "    \"\"\"\n",
    "    This computes the Laplace approximation to the marginal likelihood,\n",
    "    as defined in the Week 5 lectures.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    Cov   : (D, D) approximate posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    log_evidence : scalar value estimating `log p(y | Phi)`\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evidence(names, values):\n",
    "    assert len(names) == len(values)\n",
    "    plt.plot(np.arange(len(names)), values, 'ko')\n",
    "    ym = plt.ylim()\n",
    "    for i, v in enumerate(values):\n",
    "        plt.plot([i,i], [ym[0],v], 'r-')\n",
    "    plt.plot(np.arange(len(names)), values, 'ko')\n",
    "    plt.xticks(np.arange(len(names)), names)\n",
    "    plt.xlabel(\"Feature Set\")\n",
    "    plt.ylabel(\"Model evidence estimate\")\n",
    "    plt.ylim(ym) \n",
    "    plt.xlim(-0.5, len(names)-0.5)\n",
    "\n",
    "print(\"Model evidence estimate (simple features):\",\n",
    "      compute_laplace_log_evidence(features_simple(X_train), y_train, w_MAP_simple, Cov_simple).item())\n",
    "\n",
    "print(\"Model evidence estimate (quadratic features):\",\n",
    "      compute_laplace_log_evidence(features_quadratic(X_train), y_train, w_MAP_quad, Cov_quad).item())\n",
    "\n",
    "plot_evidence((\"simple\", \"quadratic\"),\n",
    "              (compute_laplace_log_evidence(features_simple(X_train), y_train, w_MAP_simple, Cov_simple).item(),\n",
    "               compute_laplace_log_evidence(features_quadratic(X_train), y_train, w_MAP_quad, Cov_quad).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b63d8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d355fa2808934801c1c2f480b80243d5",
     "grade": true,
     "grade_id": "B-test-evidence",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847aee57",
   "metadata": {},
   "source": [
    "# TASK #5 (2 points): Define your own feature space\n",
    "\n",
    "Your next task is to define your own, custom feature space! This can be practically any deterministic function from $\\mathbb{R}^2$ to $\\mathbb{R}^D$, and you even get to pick the dimensionality $D$.\n",
    "\n",
    "In the following cells, you first define your feature space, and then we estimate the posterior using your code above. We report training accuracy, test accuracy, and model evidence, as well as plot the decision boundaries.\n",
    "\n",
    "Feel free to get quite creative here! However, you will be asked to defend your choice of feature space in the free-answer section at the bottom.\n",
    "\n",
    "**Make sure when you submit, you include whatever you consider the \"best\" possible choice!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73ef81",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "325d61748c236b889508938fd74f8cbb",
     "grade": false,
     "grade_id": "custom-features",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features_student(X):\n",
    "    \"\"\"\n",
    "    Compute your own, custom set of features!\n",
    "    \n",
    "    INPUT:\n",
    "    X      : (N, 2) tensor of raw input data\n",
    "    \n",
    "    OUTPUT:\n",
    "    Phi    : (N, D) tensor of transformed inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef001991",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_student, losses = find_MAP(features_student(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = (predict_probs_MAP(features_student(X_train), w_MAP_student).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_student(X_test), w_MAP_student).round() == y_test).float().mean()\n",
    "print(\"YOUR features! Training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234be4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_student = compute_laplace_Cov(features_student(X_train), y_train, w_MAP_student)\n",
    "\n",
    "print(\"YOUR features! Model evidence estimate:\",\n",
    "      compute_laplace_log_evidence(features_student(X_train), y_train, w_MAP_student, Cov_student).item())\n",
    "\n",
    "plot_evidence((\"simple\", \"quadratic\", \"YOURS\"),\n",
    "              (compute_laplace_log_evidence(features_simple(X_train), y_train, w_MAP_simple, Cov_simple).item(),\n",
    "               compute_laplace_log_evidence(features_quadratic(X_train), y_train, w_MAP_quad, Cov_quad).item(),\n",
    "               compute_laplace_log_evidence(features_student(X_train), y_train, w_MAP_student, Cov_student).item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"YOUR features! MAP estimate\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_student(X), w_MAP_student))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"YOUR features! Laplace estimate\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_student(X), w_MAP_student, Cov_student))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294930f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b66d8de81acc81add0f4b1b00fcfbb9b",
     "grade": false,
     "grade_id": "cell-260c2d89b6bc0c40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Ignore the following cells. They are used by the grading system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8d628",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f5e8fe46afe84edee037f19adfd74ed",
     "grade": true,
     "grade_id": "B-features-student",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f7f9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "541fbece4305982f65dad80d55797b7c",
     "grade": true,
     "grade_id": "B-features-actuals",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f1259-b4fe-447b-b794-3616dcf56c54",
   "metadata": {},
   "source": [
    "# TASK #6 (4 points): Deep learning for features\n",
    "\n",
    "Now that you have probably decided that choosing features by hand is annoying, let's see if we can instead learn good representations directly.\n",
    "\n",
    "To do this, we define a network in two parts: a `FeatureNetwork` module, which takes the data and returns a `D`-dimensional feature embedding, and a `nn.Linear` layer for prediction.\n",
    "\n",
    "In this coursework, we will do a simple two-stage approach where first we train the model end-to-end; then, we use the pre-trained features from that model in our Laplace approximation code from the previous section. This is perhaps less robust than doing a Laplace approximation across all parameters (it is only a Laplace approximation on the last linear layer), but it lets us use all our previous code.\n",
    "\n",
    "You should implement two things:\n",
    "\n",
    "1. The `FeatureNetwork` module itself. This should take a 2d input, and return an output of dimension `output_size`.\n",
    "2. A function `train_deep_model()`, which fits the deep learning model in whatever manner you deem appropriate. Generally, I recommend fitting `model` to the training data, in order to learn the `feature_network`. You can use any choice of optimiser, parameters, early stopping, etc. **You should discuss these choices, and why, in the free response at the end.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba17a9-3df7-4c53-8b5a-fc2d668c049c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbaa3a69e7c01dded733a9de1017cb8b",
     "grade": false,
     "grade_id": "feature-network-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class FeatureNetwork(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\" Initialise a network \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        ## EXAMPLE:\n",
    "        # self.net = nn.Sequential( ???????? )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Run the forward model computation \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        ## EXAMPLE:\n",
    "        # value = ???????\n",
    "        assert value.shape == (x.shape[0], self.output_size)\n",
    "        return value\n",
    "\n",
    "\n",
    "# Feel free to change this (larger or smaller!) if you think appropriate!\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "feature_network = FeatureNetwork(OUTPUT_SIZE)\n",
    "\n",
    "# This concatenates the features from the feature network into a linear layer for prediction\n",
    "model = nn.Sequential(feature_network, nn.Linear(feature_network.output_size, 1))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b98760-6972-4868-a136-386224bd4241",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "929cef8cc73efeb7157a5f853b8717d9",
     "grade": false,
     "grade_id": "deep-training-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_deep_model():\n",
    "    \"\"\" Train your model, and learn the features.\n",
    "\n",
    "        This should update the model `model` using a pytorch optimiser, on the training set.\n",
    "\n",
    "        Feel free to use any optimiser and regularisation techniques you think appropriate.\n",
    "    \"\"\"\n",
    "\n",
    "    # You can iterate through this with `for X, y in dataloader:`\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=5)\n",
    "    \n",
    "    # You might prefer to use Adam for the deep network\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "train_deep_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777ae6e-9ca0-4a3b-bd31-a4208de5b37b",
   "metadata": {},
   "source": [
    "### The following two lines take the trained feature network, and use them as an input to your existing MAP estimation code.\n",
    "\n",
    "Your model should get a decent training accuracy (well over 80%) if everything went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da126bc-40fd-47dc-bbc7-89e93a2414b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi_deep = feature_network(X_train).detach()\n",
    "w_MAP_deep, losses = find_MAP(Phi_deep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac70e-14a7-4958-a938-76d74a3ca1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "print(\"Training accuracy:\", \n",
    "      (predict_bayes(feature_network(X_train).detach(), w_MAP_deep, Cov_deep).round() == y_train).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf6fba-0009-41c7-8794-a666bfd778f5",
   "metadata": {},
   "source": [
    "### Now, fit a Laplace approximation at the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be4903e-52fd-4359-879d-9d33389a5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_deep = compute_laplace_Cov(Phi_deep, y_train, w_MAP_deep)\n",
    "compute_laplace_log_evidence(Phi_deep, y_train, w_MAP_deep, Cov_deep).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a27979-ce76-4c4e-bc6d-51c981aa2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEEP features! Model evidence estimate:\",\n",
    "      compute_laplace_log_evidence(Phi_deep, y_train, w_MAP_deep, Cov_deep).item())\n",
    "\n",
    "plot_evidence((\"simple\", \"quadratic\", \"YOURS\", \"deep\"),\n",
    "              (compute_laplace_log_evidence(features_simple(X_train), y_train, w_MAP_simple, Cov_simple).item(),\n",
    "               compute_laplace_log_evidence(features_quadratic(X_train), y_train, w_MAP_quad, Cov_quad).item(),\n",
    "               compute_laplace_log_evidence(features_student(X_train), y_train, w_MAP_student, Cov_student).item(),\n",
    "               compute_laplace_log_evidence(Phi_deep, y_train, w_MAP_deep, Cov_deep).item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27231da7-795c-4aeb-a177-aeeda66255c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"DEEP features! MAP estimate\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(feature_network(X).detach(), w_MAP_deep))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"DEEP features! Laplace estimate\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(feature_network(X).detach(), w_MAP_deep, Cov_deep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b23cfb-7f97-4604-9abd-50083b92c8ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77208364f6c81c6a7f851208fc7f9b6f",
     "grade": true,
     "grade_id": "deep-features-laplace-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643170a-10ad-417b-8e82-4f7cf4452c3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce19739b3abbaa081b004f32f4964dd",
     "grade": true,
     "grade_id": "deep-train-acc-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b42b6-19d0-4e11-86fe-5c3ce3e1024c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e98bd77b8c69fce5be6a8ca6db55142",
     "grade": true,
     "grade_id": "deep-test-acc-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64ae0b-16ad-4e80-aef3-6b13731a2d99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a59ab4ba42e52251cd6aa614c7308de",
     "grade": false,
     "grade_id": "cell-b3e5f8853d14b81a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Short Answer (5 points)\n",
    "\n",
    "Please answer the following questions, now that you have compared your trained features and your hand-learned features, in **at most 150 words each**.\n",
    "\n",
    "1. Describe your hand-chosen features. How did you decide or evalaute whether they are good features?\n",
    "2. Describe your deep learning model. How did you select this model, and this training strategy? What design choices did you need to make (for either the architecture, or for training)? Are you satisfied with the results?\n",
    "3. Are your deep learning features better than the hand-selected features? Why or why not? Which features would you use for later prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688ef94-ef51-4e5e-a89f-6dbbb932142f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2400d23174a974a20bef435f7741d89",
     "grade": true,
     "grade_id": "short-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
